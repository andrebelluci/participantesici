# ============= ROBOTS.TXT - BLOQUEIO TOTAL =============
# Arquivo: public_html/robots.txt
#
# Este arquivo bloqueia TODOS os bots de busca e crawlers
# do sistema de gestão de participantes

# ============= BLOQUEIO GERAL =============
User-agent: *
Disallow: /

# ============= BOTS ESPECÍFICOS =============
# Google
User-agent: Googlebot
Disallow: /

User-agent: Googlebot-Image
Disallow: /

User-agent: Googlebot-News
Disallow: /

User-agent: Googlebot-Video
Disallow: /

# Bing
User-agent: Bingbot
Disallow: /

User-agent: BingPreview
Disallow: /

# Yahoo
User-agent: Slurp
Disallow: /

# DuckDuckGo
User-agent: DuckDuckBot
Disallow: /

# Yandex
User-agent: YandexBot
Disallow: /

# Baidu
User-agent: Baiduspider
Disallow: /

# Facebook
User-agent: facebookexternalhit
Disallow: /

# Twitter
User-agent: Twitterbot
Disallow: /

# WhatsApp
User-agent: WhatsApp
Disallow: /

# Outros crawlers comuns
User-agent: ia_archiver
Disallow: /

User-agent: Wayback
Disallow: /

User-agent: archive.org_bot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

# ============= SEM SITEMAP =============
# Não fornece sitemap para evitar indexação

# ============= CRAWL-DELAY =============
# Adiciona delay para desencorajar crawling
Crawl-delay: 86400

# ============= COMENTÁRIO EXPLICATIVO =============
# Este é um sistema interno de gestão de participantes
# Não destinado à indexação pública
# Acesso restrito apenas a usuários autorizados